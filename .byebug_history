exit
NormalizeUrl.process(URI.join(self.host,link['href']).to_s)
c
URI.join(self.host,link['href']).to_s
c
ur.host != URI.parse(self.host).host
ur.host
self.host
ur = URI.parse(URI.escape(link["href"]))
doc
doc.css('a')
ur = URI.parse(URI.escape(link["href"]))
c
ur = URI.parse(URI.escape(link["href"]))
exit
c
doc.css('a').size
doc.css('a')
response.body
response
request
self.urls
c
Typhoeus.get("https://www.crawler-test.com")
response
response.body
doc.css('a')
response.code
self.urls
exit
response.headers["LOCATION"]
response.body
response.code
response
url
res
c
(canon and (!canon.text.strip.blank?))
self.urls[url]
canon.text
canon = doc.xpath('//link[@rel="canonical"]/@href')
doc = Nokogiri::HTML(response.body)
response.code.to_s
exit
exitr
canon
exit
response.body
url
canon
exit
response.body
canon
exit
self.urls["http://pathofast-local/#!"]
exit
self.urls_from_file
self.seed_urls.size
self.urls_from_file.size
url
self.urls_from_file.size
self.urls_from_file
exit
puts url
c
puts url
c
puts url
exit
url
exit
puts self.urls_from_file
exit
self.urls_from_file.include? url
self.urls_from_file
url
exit
self.urls_from_file
url
urls[3]
urls[2]
urls[1]
urls[]
urls[0]
urls[3]
urls[4]
exit
row.to_h
exit
urls.to_s
urls[1]
urls[0]
urls.size
data[1]
data.first
exit
c
l.split(",")[0]
l
exit
c
self.opts["urls_file"]
exit
response.headers
response.code.to_s
exit
c
SecureRandom.uuid
SecureRandom
exit
urls_hash[url].keys
urls_hash[url]
url
exit
cexit
c
self.completed_urls
exit
response.effective_url
request.effective_url
request
request.url
self.urls
exit
response
response.code
response.headers
c
exit
c
exit
c
exit
c
exit
c
exit
self.completed_urls.size
self
crawler
c
